{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re\n",
    "\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer,SnowballStemmer\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_news_orig = pd.read_csv(\"../data/real_news.csv\") # https://github.com/cuilimeng/CoAID/blob/master/07-01-2020/NewsRealCOVID-19.csv\n",
    "df_fake_news_orig = pd.read_csv(\"../data/fake_news.csv\") # https://github.com/cuilimeng/CoAID/blob/master/07-01-2020/NewsFakeCOVID-19.csv\n",
    "\n",
    "df_real_news_orig = pd.DataFrame({\"title\": df_real_news_orig['title'] + df_real_news_orig['content'].replace(np.nan, ' NULL', regex=True)})\n",
    "df_fake_news_orig = pd.DataFrame({\"title\": df_fake_news_orig['title'] + df_fake_news_orig['content'].replace(np.nan, ' NULL', regex=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real_news = pd.concat([df_real_news_orig[['title']], pd.DataFrame({\"label\": np.ones((len(df_real_news_orig)), dtype=int)})], axis=1).reset_index(drop=True)[:len(df_fake_news_orig)]\n",
    "df_fake_news = pd.concat([df_fake_news_orig[['title']], pd.DataFrame({\"label\": np.zeros((len(df_fake_news_orig)), dtype=int)})], axis=1).reset_index(drop=True)\n",
    "df_news = pd.concat([df_real_news, df_fake_news], axis=0).reset_index(drop=True)\n",
    "df_news.to_csv(\"../data/covid_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Define function for removing special characters\n",
    "# def remove_special_characters(text, remove_digits=True):\n",
    "#     pattern=r'[^a-zA-z0-9\\s]'\n",
    "#     text=re.sub(pattern, \" \", text)\n",
    "#     return text\n",
    "\n",
    "# #Stemming the text\n",
    "# def simple_lemmatizer(text):\n",
    "#     ps = WordNetLemmatizer()\n",
    "#     text= ' '.join([ps.lemmatize(word.lower(), pos=\"v\") for word in text.split()])\n",
    "#     return text\n",
    "\n",
    "# #set stopwords to english\n",
    "# stop=set(stopwords.words('english'))\n",
    "# # print(stop)\n",
    "\n",
    "# #removing the stopwords\n",
    "# def remove_stopwords(text, is_lower_case=True):\n",
    "#     #Tokenization of text\n",
    "#     tokenizer = ToktokTokenizer()\n",
    "#     #Setting English stopwords\n",
    "#     stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     tokens = [token.strip() for token in tokens]\n",
    "#     if is_lower_case:\n",
    "#         filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "#     else:\n",
    "#         filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "\n",
    "#     filtered_text = ' '.join(filtered_tokens)    \n",
    "#     return filtered_text\n",
    "\n",
    "# def pipelinize(function, active=True):\n",
    "#     def list_comprehend_a_function(list_or_series, active=True):\n",
    "#         if active:\n",
    "#             return [function(i) for i in list_or_series]\n",
    "#         else: # if it's not active, just pass it right back\n",
    "#             return list_or_series\n",
    "#     return FunctionTransformer(list_comprehend_a_function, validate=False, kw_args={'active':active})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# nltk.download('punkt')\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators = [('remove_special_chars', pipelinize(remove_special_characters)),\n",
    "#              ('simple_lemmatizer', pipelinize(simple_lemmatizer)),\n",
    "#              ('remove_stopwords', pipelinize(remove_stopwords)),\n",
    "#              ('count_vectorizer', CountVectorizer(ngram_range=(1,1)))]\n",
    "CountVectorizer()\n",
    "estimators = [\n",
    "             ('count_vectorizer', CountVectorizer(ngram_range=(1,1), stop_words=\"english\", lowercase=True, tokenizer=LemmaTokenizer()))]\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/covid_news.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title'], df['label'], test_size=0.2, random_state=42) \n",
    "X_train_all = pd.concat([X_train, y_train], axis=1)\n",
    "X_test_all = pd.concat([X_test, y_test], axis=1)\n",
    "X_train_all.to_csv(\"../data/train_news.csv\", index=False)\n",
    "X_test_all.to_csv(\"../data/test_news.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kelvinsoh/opt/anaconda3/envs/virtualenv/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X_train_ = pipeline.fit_transform(X_train)\n",
    "X_test_ = pipeline.transform(X_test)\n",
    "\n",
    "y_train_ = y_train\n",
    "y_test_ = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (425, 3553)\n",
      "BOW_cv_test: (107, 3553)\n"
     ]
    }
   ],
   "source": [
    "print('BOW_cv_train:', X_train_.shape)\n",
    "print('BOW_cv_test:', X_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimators_tfidf = [('remove_special_chars', pipelinize(remove_special_characters)),\n",
    "#              ('simple_lemmatizer', pipelinize(simple_lemmatizer)),\n",
    "#              ('remove_stopwords', pipelinize(remove_stopwords)),\n",
    "#              ('tfidf_vectorizer', TfidfVectorizer(use_idf=True, ngram_range=(1,1)))]\n",
    "estimators_tfidf = [\n",
    "             ('tfidf_vectorizer', TfidfVectorizer(use_idf=True, ngram_range=(1,1), stop_words=\"english\", lowercase=True, tokenizer=LemmaTokenizer()))]\n",
    "pipeline_tfidf = Pipeline(estimators_tfidf)\n",
    "\n",
    "X_train_tfidf = pipeline_tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = pipeline_tfidf.transform(X_test)\n",
    "\n",
    "y_train_tfidf = y_train\n",
    "y_test_tfidf = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    59\n",
       "0    48\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tfidf.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    218\n",
       "1    207\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tfidf.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (425, 3553)\n",
      "Tfidf_test: (107, 3553)\n"
     ]
    }
   ],
   "source": [
    "print('Tfidf_train:',X_train_tfidf.shape)\n",
    "print('Tfidf_test:',X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9158878504672897"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2',max_iter=1000,C=1,random_state=42)\n",
    "lr_bow = lr.fit(X_train_, y_train_)\n",
    "\n",
    "lr_bow_predict = lr.predict(X_test_)\n",
    "accuracy_score(lr_bow_predict, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9345794392523364"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_tfidf = LogisticRegression(penalty='l2',max_iter=1000, C=1, random_state=42)\n",
    "lr_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "lr_tfidf_predict = lr_tfidf.predict(X_test_tfidf)\n",
    "accuracy_score(lr_tfidf_predict, y_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Prediction (LogisticRegression (TFIDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71342251, 0.28657749],\n",
       "       [0.57705878, 0.42294122],\n",
       "       [0.26054912, 0.73945088]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = [\"Real news for donald trump NULL covid context cdc facebook\", \"Hello how are you?\", \"What New CDC Guidelines Mean for Workplaces as They Reopenexperts say business owners should adopt safety practices that best fit their workplace during the covid-19 pandemic. getty images experts say there are many challenges to reopening workplaces during the covid-19 pandemic even with the release of federal guidelines. they advise business owners to use the new recommendations simply as guideposts and to adopt measures that best fit their workplace and community. making employees feel safe is a key ingredient to a successful reopening say ex. \"]\n",
    "input_text_transform = pipeline_tfidf.transform(input_text)\n",
    "lr_tfidf.predict_proba(np.array(input_text_transform.todense()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9345794392523364"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_tfidf = RandomForestClassifier(random_state=42)\n",
    "rf_tfidf.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "rf_tfidf_predict = rf_tfidf.predict(X_test_tfidf)\n",
    "accuracy_score(rf_tfidf_predict, y_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix (LogisticRegression TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43,  2],\n",
       "       [ 5, 57]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(lr_tfidf_predict, y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report (LogisticRegression TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.92        48\n",
      "           1       0.92      0.97      0.94        59\n",
      "\n",
      "    accuracy                           0.93       107\n",
      "   macro avg       0.94      0.93      0.93       107\n",
      "weighted avg       0.94      0.93      0.93       107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_, lr_tfidf_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC (LogisticRegression TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766949152542372"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(np.array(y_test_tfidf), lr_tfidf.predict_proba(X_test_tfidf)[:, 1])\n",
    "metrics.auc(fpr, tpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
